<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="description" content="_your description goes here_" />
<meta name="keywords" content="_your,keywords,goes,here_" />
<meta name="author" content="_your name goes here_  / Original design: Andreas Viklund - http://andreasviklund.com/" />
<link rel="stylesheet" type="text/css" href="andreas01.css" media="screen" title="andreas01 (screen)" />
<link rel="stylesheet" type="text/css" href="print.css" media="print" />
<title>Allan's Homepage</title>

  <SCRIPT>
  function show(c_Str,imgg)
  {if(document.all(c_Str).style.display=='none')
  {document.all(c_Str).style.display='block';
  document.all(imgg).src='http://www.webym.net/jiaocheng/upfile/jscontent/12493937251.gif'}
  else{document.all(c_Str).style.display='none';
  document.all(imgg).src='http://www.webym.net/jiaocheng/upfile/jscontent/12493937250.gif'}}
  </SCRIPT>


<SCRIPT language=JavaScript>
  function preview(){
  pr=window.open('','Preview','scrollbars=1,menubar=1,status=1,width=480 height=320,left=10,top=10');
  pr.document.write(jstxt.innerText);}
  </SCRIPT>

<style type="text/css"> .firstLevel {   list-style: none;     } .secondLevel {    display: none;  } #first1:hover ul, #first2:hover ul, #first3:hover ul {    display: block; } 

</style>

</head>

<body><div id="wrap">

<div id="header">
</div>

<img id="frontphoto" src="logo.png" width="1060" height="175" alt="" />

<div id="avmenu">
<h2 class="hide">Full Publication:</h2>
<ul class="firstLevel">
<li><a href="index.html">Home</a></li>
<li><a href="research.html">Research</a></li>
<li><a href="news.html">News</a></li>
<li id="first1"><a>Full Publication</a>
<ul class="secondLevel">
<li><a href="Journal.html"> * Journal</a></li>
<li><a href="Conference.html"> * Conference</a></li>
<li><a href="Others.html"> * Others</a></li>
</ul></li>
<li><a href="Course.html">Course</a></li>
<li><a href="service.html">Service</a></li>
<li><a href="award.html">Award</a></li>
</ul>
</div>


<div id="extras">
<h3>Links:</h3>
<p><a href="https://scholar.google.com/citations?user=TKbyRRsAAAAJ&hl=en" target="_blank">Google Scholar</a><p>
<p><a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Zhengming" target="_blank">DBLP</a><p>
<p><a href="https://www.linkedin.com/feed/?trk=nav_logo" target="_blank">LinkedIn</a><p>
<p><a href="https://github.com/allanding">GitHub</a></p>
 </div>


<div id="content">
  <h5>Research Interests</h5>
  <p3>My research interest lies in computer vision and machine learning, with a focus on developing scalable algorithms to learn robust representations from large-scale data.</p3><br>


  <table width="96%" height="18" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr> 
  <td> </td>
  </tr>
  <tr> 
  <td> <TABLE 
  style="PADDING-RIGHT: 2px; PADDING-LEFT: 2px; PADDING-BOTTOM: 2px; PADDING-TOP: 2px" 
  cellSpacing=1 cellPadding=0 width="100%" border=0>
  <TBODY>
  <TR> 
  <TD height=16 
  style="BORDER-RIGHT: #cccccc 0px solid; BORDER-TOP: #cccccc 0px solid; PADDING-LEFT: 4px; BORDER-LEFT: #cccccc 0px solid; CURSOR: hand; COLOR: #333333; BORDER-BOTTOM: #cccccc 0px solid; BACKGROUND-COLOR: #ffffff" 
  onclick='show("tip432","img432")'><IMG 
  id=img432 hspace=1 src="http://www.webym.net/jiaocheng/upfile/jscontent/12493937250.gif" align=absMiddle 
  border=0> <p3>Transfer Learning</p3>
  <TABLE cellSpacing=0 cellPadding=0 width=100% align=center border=0>
  <TBODY>
  <TR> 
  <TD width="20" 
  height=1 colSpan=2 background=images/bg-line.gif></TD>
  </TR>
  </TBODY>
  </TABLE></TD>
  </TR>
  <TR> 
  <TD id=tip432 
  style="PADDING-RIGHT: 1px; DISPLAY: none; PADDING-LEFT: 1px; PADDING-BOTTOM: 1px; PADDING-TOP: 1px" 
  width="100%"> <DIV 
  style="BORDER-RIGHT: #808080 0px solid; PADDING-RIGHT: 4px; BORDER-TOP: #808080 0px solid; PADDING-LEFT: 4px; PADDING-BOTTOM: 4px; BORDER-LEFT: #808080 0px solid; LINE-HEIGHT: 150%; PADDING-TOP: 4px; BORDER-BOTTOM: #808080 0px solid; BACKGROUND-COLOR: #f7f7ef">

<div align="center" vertical-align="middle">
            Missing Modality Transfer Learning
</div><br>

<img id="mmtl" src="figure/mmtl.png" width="670" height="120" alt="" text-align="center"  />

<strong>[Introduction]</strong><br>

<font size="2" color="black">Transfer learning is usually exploited to leverage previously well-learned source domain for evaluating the unknown target domain; however, it may fail if no target data are available in the training stage. This problem arises when the data are multi-modal. For example, the target domain is in one modality while the source domain is in another. To overcome this, we first borrow an auxiliary database with complete modalities, then consider knowledge transfer across databases and across modalities within databases simultaneously in a single framework. The contributions are threefold: 1) a latent factor is introduced to uncover the underlying structure of the missing modality from the known data; 2) transfer learning in two directions allows the data alignment between both modalities and databases, giving rise to a very promising recovery; 3) an efficient solution with theoretical guarantees to the proposed latent low-rank transfer learning algorithm. </font>

<br>

<strong>[Related Work]</strong><br>

<font size="2" color="black">1. <b>Zhengming Ding</b>, Ming Shao, and Yun Fu. <i>Missing Modality Transfer Learning via Latent Low-Rank Constraint</i>. IEEE Transactions on Image Processing (<b>TIP</b>), vol. 24, no. 11, pp. 4322-4334, 2015.

<br>
2. <b>Zhengming Ding</b>, Ming Shao and Yun Fu. <i>Latent Low-Rank Transfer Subspace Learning for Missing Modality Recognition</i>. Twenty-Eighth AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2014.</font>
<hr size=3>


<div align="center" vertical-align="middle">
            Incomplete Multi-Source Transfer Learning
        </div><br>

  <img id="imtl" src="figure/imtl.png" width="540" height="240" alt="" text-align="center" />
  <br>

<strong>[Introduction]</strong><br>

<font size="2" color="black">Transfer learning is generally exploited to adapt well-established source knowledge for learning tasks in weakly labeled or unlabeled target domain. Nowadays, it is common to see multiple sources  available for knowledge transfer, each of which, however, may not include complete classes information of the target domain. Naively merging multiple sources together would lead to inferior results due to the large divergence among multiple sources. In this paper, we attempt to utilize incomplete multiple sources for effective knowledge transfer to facilitate the learning task in target domain. To this end, we propose an Incomplete Multi-source Transfer Learning (IMTL) through two directional knowledge transfer, i.e., cross-domain transfer from each source to target, and cross-source transfer. Specifically, in cross-domain direction, we deploy latent low-rank transfer learning guided by iterative structure learning to transfer knowledge from each single source to target domain. This practice reinforces to compensate for any missing data in each source by the complete target data. Whilst in cross-source direction, unsupervised manifold regularizer as well as effective multi-source alignment are explored to jointly compensate for missing data from one portion of source to another. </font>

<br>

<strong>[Related Work]</strong><br>

<font size="2" color="black">
<b>Zhengming Ding</b>, Ming Shao, and Yun Fu. Incomplete Multisource Transfer Learning. IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>), vol. 29, no. 2, pp. 310-323, 2018.</font>



</DIV></TD>
  </TR>
  <br>
  <TR> 
  <TD height=16 
  style="BORDER-RIGHT: #cccccc 0px solid; BORDER-TOP: #cccccc 0px solid; PADDING-LEFT: 4px; BORDER-LEFT: #cccccc 0px solid; CURSOR: hand; COLOR: #333333; BORDER-BOTTOM: #cccccc 0px solid; BACKGROUND-COLOR: #ffffff" 
  onclick='show("tip331","img331")'><IMG 
  id=img331 hspace=1 src="http://www.webym.net/jiaocheng/upfile/jscontent/12493937250.gif" align=absMiddle 
  border=0>  <p3>Multi-view Learning</p3>
  <TABLE cellSpacing=0 cellPadding=0 width=100% align=center border=0>
  <TBODY>
  <TR> 
  <TD width="20" 
  height=1 colSpan=2 background=images/bg-line.gif></TD>
  </TR>
  </TBODY>
  </TABLE></TD>
  </TR>
  <TR> 
  <TD id=tip331 
  style="PADDING-RIGHT: 1px; DISPLAY: none; PADDING-LEFT: 1px; PADDING-BOTTOM: 1px; PADDING-TOP: 1px" 
  width="100%"> <DIV 
  style="BORDER-RIGHT: #808080 0px solid; PADDING-RIGHT: 4px; BORDER-TOP: #808080 0px solid; PADDING-LEFT: 4px; PADDING-BOTTOM: 4px; BORDER-LEFT: #808080 0px solid; LINE-HEIGHT: 150%; PADDING-TOP: 4px; BORDER-BOTTOM: #808080 0px solid; BACKGROUND-COLOR: #f7f7ef">

<div align="center" vertical-align="middle">
            Low-rank Common Subspace Learning 
        </div><br>

  <img id="lrcs" src="figure/lrcs.png" width="670" height="300" alt="" />

<strong>[Introduction]</strong><br>

<font size="2" color="black">Multi-view data are of great abundance in real-world applications, since various view-points and multiple sensors desire to represent the data in a better way. Conventional multi-view learning methods aimed to learn multiple view-specific transformations meanwhile assumed the view knowledge of training and test data were available in advance. However, they would fail when we do not have any prior knowledge for the probe data's view information, since the correct view-specific projections cannot be utilized to extract effective feature representations. In this paper, we develop a Collective Low-Rank Subspace (CLRS) algorithm to deal with this problem in multi-view data analysis. CLRS attempts to reduce the semantic gap across multiple views through seeking a view-free low-rank projection shared by multiple view-specific transformations. Moreover, we exploit low-rank reconstruction to build a bridge between the view-specific features and those view-free ones transformed with the collective low-rank subspace. Furthermore, a supervised cross-view regularizer is developed to couple the within-class data across different views to make the learned collective subspace more discriminative.</font>

<br>

<strong>[Related Work]</strong><br>

<font size="2" color="black">
1. <b>Zhengming Ding</b>, and Yun Fu. Robust Multi-view Data Analysis through Collective Low-Rank Subspace.
IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>), vol. 29, no. 5, pp. 1986-1997, 2018.<br>
2. <b>Zhengming Ding</b>, Yun Fu. Low-Rank Common Subspace for Multi-View Learning. IEEE International
Conference on Data Mining (<b>ICDM</b>), 2014.</font>
<hr size=3>



</DIV></TD>
  </TR>
  <TR> 
  <TD height=16 
  style="BORDER-RIGHT: #cccccc 0px solid; BORDER-TOP: #cccccc 0px solid; PADDING-LEFT: 4px; BORDER-LEFT: #cccccc 0px solid; CURSOR: hand; COLOR: #333333; BORDER-BOTTOM: #cccccc 0px solid; BACKGROUND-COLOR: #ffffff" 
  onclick='show("tip247","img247")'><IMG 
  id=img247 hspace=1 src="http://www.webym.net/jiaocheng/upfile/jscontent/12493937250.gif" align=absMiddle 
  border=0> <p3> Robust Feature Learning</p3>

  <TABLE cellSpacing=0 cellPadding=0 width=100% align=center border=0>
  <TBODY>
  <TR> 
  <TD width="20" 
  height=1 colSpan=2 background=images/bg-line.gif></TD>
  </TR>
  </TBODY>
  </TABLE></TD>
  </TR>
  <TR> 
  <TD id=tip247 
  style="PADDING-RIGHT: 1px; DISPLAY: none; PADDING-LEFT: 1px; PADDING-BOTTOM: 1px; PADDING-TOP: 1px" 
  width="100%"> <DIV 
  style="BORDER-RIGHT: #808080 0px solid; PADDING-RIGHT: 4px; BORDER-TOP: #808080 0px solid; PADDING-LEFT: 4px; PADDING-BOTTOM: 4px; BORDER-LEFT: #808080 0px solid; LINE-HEIGHT: 150%; PADDING-TOP: 4px; BORDER-BOTTOM: #808080 0px solid; BACKGROUND-COLOR: #f7f7ef">

<div align="center" vertical-align="middle">
            Deep Robust Encoder 
 <br>

  <img id="dre" src="figure/dre.png" width="460" height="400" alt="" />       </div><br>


<strong>[Introduction]</strong><br>

<font size="2" color="black">Deep learning has attracted increasing attentions recently due to its appealing performance in various tasks. As a principal way of deep feature learning, deep auto-encoder has been widely discussed in such problems as dimensionality reduction and model pre-training. Conventional auto-encoder and its variants usually involve additive noises (e.g., Gaussian, masking) for training data to learn robust features, which, however, did not consider the already corrupted data. In this paper, we propose a novel Deep Robust Encoder (DRE) through locality preserving low-rank dictionary to extract robust and discriminative features from corrupted data, where a low-rank dictionary and a regularized deep auto-encoder are jointly optimized. First, we propose a novel loss function in the output layer with a learned low-rank clean dictionary and corresponding weights with locality information, which ensures that the reconstruction is noise free. Second, discriminant graph regularizers that preserve the local geometric structure for the data are developed to guide the deep feature learning in each encoding layer.</font>

<br>

<strong>[Related Work]</strong><br>

<font size="2" color="black">
<b>Zhengming Ding</b>, Ming Shao and Yun Fu. Deep Robust Encoder through Locality Preserving Low-Rank Dictionary. European Conference on Computer Vision, (<b>ECCV</b>), 2016.</font>

<!-- <hr size=3> -->

</DIV></TD>
  </TR>
  <TR> 
  <TD height=16 
  style="BORDER-RIGHT: #cccccc 0px solid; BORDER-TOP: #cccccc 0px solid; PADDING-LEFT: 4px; BORDER-LEFT: #cccccc 0px solid; CURSOR: hand; COLOR: #333333; BORDER-BOTTOM: #cccccc 0px solid; BACKGROUND-COLOR: #ffffff" 
  onclick='show("tip166","img166")'><IMG 
  id=img166 hspace=1 src="http://www.webym.net/jiaocheng/upfile/jscontent/12493937250.gif" align=absMiddle 
  border=0><p3> Zero-Shot Learning</p3>
  <TABLE cellSpacing=0 cellPadding=0 width=100% align=center border=0>
  <TBODY>
  <TR> 
  <TD width="20" 
  height=1 colSpan=2 background=images/bg-line.gif></TD>
  </TR>
  </TBODY>
  </TABLE></TD>
  </TR>
  <TR> 
  <TD id=tip166 
  style="PADDING-RIGHT: 1px; DISPLAY: none; PADDING-LEFT: 1px; PADDING-BOTTOM: 1px; PADDING-TOP: 1px" 
  width="100%"> <DIV 
  style="BORDER-RIGHT: #808080 0px solid; PADDING-RIGHT: 4px; BORDER-TOP: #808080 0px solid; PADDING-LEFT: 4px; PADDING-BOTTOM: 4px; BORDER-LEFT: #808080 0px solid; LINE-HEIGHT: 150%; PADDING-TOP: 4px; BORDER-BOTTOM: #808080 0px solid; BACKGROUND-COLOR: #f7f7ef">

<div align="center" vertical-align="middle">
            Generative Zero-Shot Learning
        </div><br>

  <img id="mmtl" src="figure/zero_shot.png" width="670" height="290" alt="" />
<p2>
<strong>[Introduction]</strong><br>

Zero-shot learning for visual recognition, which approaches identifying unseen categories through a shared visual-semantic function learned on the seen categories and is expected to well adapt to unseen categories, has received considerable research attention most recently. However, the semantic gap between discriminant visual features and their underlying semantics is still the biggest obstacle, because there usually exists domain disparity across the seen and unseen classes. To deal with this challenge, we design two-stage generative adversarial networks to enhance the generalizability of semantic dictionary through low-rank embedding for zero-shot learning. In detail, we formulate a novel framework to simultaneously seek a two-stage generative model and a semantic dictionary to connect visual features with their semantics under a low-rank embedding. Our first-stage generative model is able to augment more semantic features for the unseen classes, which are then used to generate more discriminant visual features in the second stage, to expand the seen visual feature space. Therefore, we will be able to seek a better semantic dictionary to constitute the latent basis for the unseen classes based on the augmented semantic and visual data.

<br><br>

<strong>[Related Work]</strong><br>

<font size="2" color="black">
1. <b>Zhengming Ding</b>, Ming Shao, and Yun Fu. Generative Zero-Shot Learning via Low-Rank Embedded Semantic Dictionary. IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2018.<br>
2. <b>Zhengming Ding</b>, Ming Shao and Yun Fu. Low-Rank Embedded Ensemble Semantic Dictionary for Zero-Shot Learning. IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017.
</font>
<!-- <hr size=3> -->



</DIV></TD>
  </TR>
  <TR> 
  <TD height=16 
  style="BORDER-RIGHT: #cccccc 0px solid; BORDER-TOP: #cccccc 0px solid; PADDING-LEFT: 4px; BORDER-LEFT: #cccccc 0px solid; CURSOR: hand; COLOR: #333333; BORDER-BOTTOM: #cccccc 0px solid; BACKGROUND-COLOR: #ffffff" 
  onclick='show("tip158","img158")'><IMG 
  id=img158 hspace=1 src="http://www.webym.net/jiaocheng/upfile/jscontent/12493937250.gif" align=absMiddle 
  border=0> <p3> One-Shot Learning </p3>
  <TABLE cellSpacing=0 cellPadding=0 width=100% align=center border=0>
  <TBODY>
  <TR> 
  <TD width="20" 
  height=1 colSpan=2 background=images/bg-line.gif></TD>
  </TR>
  </TBODY>
  </TABLE></TD>
  </TR>
  <TR> 
  <TD id=tip158 
  style="PADDING-RIGHT: 1px; DISPLAY: none; PADDING-LEFT: 1px; PADDING-BOTTOM: 1px; PADDING-TOP: 1px" 
  width="100%"> <DIV 
  style="BORDER-RIGHT: #808080 0px solid; PADDING-RIGHT: 4px; BORDER-TOP: #808080 0px solid; PADDING-LEFT: 4px; PADDING-BOTTOM: 4px; BORDER-LEFT: #808080 0px solid; LINE-HEIGHT: 150%; PADDING-TOP: 4px; BORDER-BOTTOM: #808080 0px solid; BACKGROUND-COLOR: #f7f7ef">

<div align="center" vertical-align="middle">
            One-Shot Face Recognition
        </div><br>

  <img id="mmtl" src="figure/one_shot.png" width="670" height="290" alt="" />
<p2>
<strong>[Introduction]</strong><br>

One-shot face recognition measures the ability to recognize persons with only seeing them once, which is a hallmark of human visual intelligence. It is challenging for existing machine learning approaches to mimic this way, since limited data cannot well represent the data variance. To this end, we propose to build a large-scale face recognizer, which is capable to fight off the data imbalance difficulty. To seek a more effective general classifier, we develop a novel generative model attempting to synthesize meaningful data for one-shot classes by adapting the data variances from other normal classes. Specifically, we formulate conditional generative adversarial networks and the general Softmax classifier into a unified framework. Such a two-player minimax optimization can guide the generation of more effective data, which benefit the classifier learning for one-shot classes.

<br><br>

<strong>[Related Work]</strong><br>

1. <b>Zhengming Ding</b>, Yandong Guo, Lei Zhang, Yun Fu. One-Shot Face Recognition via Generative Learning, IEEE Conference on Automatic Face and Gesture Recognition (FG), 2018.


</DIV></TD>
  </TR>
  </TBODY>
  </TABLE></td>
  </tr>
  </table>

<br>

<div id="Click">




</ul>

</div>
</div>



<div id="footer">
Copyright &copy; 2018 <a href="http://allanding.net/">Zhengming Ding, Northeastern University.</a>
</div>

</div>



</body>
</html>
